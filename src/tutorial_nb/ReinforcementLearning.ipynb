{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Deep Q Reinforcement Learning](http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook based of linked tutorial. Started adjusting things for our application. General work flow, we are going to create a neural network which takes in features(these could be past prices, technical indicators,etc) for now this will be the 10 past values, we are going to then create a network to create the Q function for each of our results(Huber Loss). The outputs to this network will be the action pairs to run. Actions will be buy sell and hold. We will need to create a reward function which we will tweak till things look good. Some of this notebook may seem blotted but a lot of the things are recommended tricks for training like maintaining replay memory and having two different networks one for training and one for actually evaluating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import financial_data as fd\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import pixiedust\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "import pdb\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "pm = fd.financial_data(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data I am using is from MSFT for learning. You can change ticker for other stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "ticker='MSFT'\n",
    "state=pm.norm_data_ls[pm.ticker_ls.index(ticker)].Close\n",
    "date=pm.norm_data_ls[pm.ticker_ls.index(ticker)].date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class that stores all of the different transitions we make and ensures that when we sample it during the training process the samples aren't correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Deep Q Learning Network. Here one inputs the features(AKA State) and these states then predict our Q function for each action. The action with the largest Q is chosen. This learns based off reward function defined later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        hidden_size = 120  # Random Parameter that can be tuned\n",
    "        actions = 3  # 3 Different Actions Buy Sell Hold\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(len(pm.x_test[1]), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, actions)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.9\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "policy_net = DQN()\n",
    "target_net = DQN()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "if use_cuda:\n",
    "    policy_net.cuda()\n",
    "    target_net.cuda()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        return policy_net(Variable(state, volatile=True).type(FloatTensor)).data.max(0)[1]\n",
    "    else:\n",
    "        return LongTensor([random.randrange(3)])\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.FloatTensor(episode_durations)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(1)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = ByteTensor(tuple(map(lambda s: s is not None, batch.next_state)))\n",
    "    non_final_next_states = Variable(torch.cat([s for s in batch.next_state if s is not None]), volatile=True)\n",
    "    non_final_next_states = non_final_next_states.view(128,10)\n",
    "    \n",
    "    state_batch = Variable(torch.cat(batch.state))\n",
    "    state_batch = state_batch.view(128,10)\n",
    "    action_batch = Variable(torch.cat(batch.action))\n",
    "    action_batch = action_batch.view(128,1)\n",
    "    reward_batch = Variable(torch.cat(batch.reward))\n",
    "    reward_batch = reward_batch.view(128,1)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = Variable(torch.zeros(BATCH_SIZE).type(Tensor))\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    next_state_values = next_state_values.unsqueeze(1)\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    # Undo volatility (which was used to prevent unnecessary gradients)\n",
    "    expected_state_action_values = Variable(expected_state_action_values.data)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(action, cur_price, next_price, days, since_buy, price_buy):\n",
    "    #Write Logic to determine if at end of time series\n",
    "    #Write Logic to figure out reward for action(% Profit?)\n",
    "    \n",
    "    # action: 1 - Buy, 2 - Sell, 3 - Hold\n",
    "    # cur_price: current price of the stock (10th value in array)\n",
    "    # next_price: next price of the stock (y value)\n",
    "    # days: day count within current episode\n",
    "    # since_buy: days since the last buy (-1 if no holdings)\n",
    "    # price_buy: price at the last buy (-1 if no holdings)\n",
    "    \n",
    "    if (action == 0):\n",
    "        #BUY\n",
    "        if (price_buy == -1):\n",
    "            if ((next_price - cur_price) > 0):\n",
    "                reward = (next_price - cur_price)*5\n",
    "                price_buy = cur_price\n",
    "                since_buy = 1\n",
    "            else:\n",
    "                reward = (next_price - cur_price)*5\n",
    "                price_buy = cur_price\n",
    "                since_buy = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "            price_buy = -1\n",
    "            since_buy = -1\n",
    "            \n",
    "    elif (action == 1):\n",
    "        #SELL\n",
    "        if ((since_buy > 0) & (price_buy > 0)):\n",
    "            reward = (cur_price - price_buy)*100\n",
    "            price_buy = -1\n",
    "            since_buy = -1\n",
    "        else:\n",
    "            reward = 0\n",
    "            price_buy = -1\n",
    "            since_buy = -1\n",
    "            \n",
    "    elif (action == 2):\n",
    "        #HOLD\n",
    "        if (price_buy == -1):\n",
    "            if ((next_price - cur_price) > 0):\n",
    "                reward = -10\n",
    "                price_buy = -1.0\n",
    "                since_buy = -1.0\n",
    "            else:\n",
    "                reward = 10\n",
    "                price_buy = -1.0\n",
    "                since_buy = -1.0\n",
    "        else:\n",
    "            if ((next_price - cur_price) > 0):\n",
    "                reward = 10*since_buy + (cur_price - price_buy)\n",
    "                price_buy = price_buy\n",
    "                since_buy = since_buy + 1\n",
    "            else:\n",
    "                reward = -2*since_buy + (cur_price - price_buy)\n",
    "                price_buy = price_buy\n",
    "                since_buy = since_buy + 1\n",
    "        \n",
    "        \n",
    "    if (days > 500):\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    \n",
    "    return reward, done, since_buy, price_buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Done\n",
      "Episode Done\n",
      "Episode Done\n",
      "Episode Done\n",
      "Episode Done\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 5\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = torch.Tensor(pm.x_test[1])\n",
    "    state.unsqueeze(0)\n",
    "    since_buy = -1.0\n",
    "    price_buy = -1.0\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        \n",
    "        action = select_action(state)\n",
    "        reward, done, since_buy, price_buy = step(action[0], pm.x_test[t][9], pm.y_test[t], t, since_buy, price_buy)\n",
    "        reward = Tensor([reward])\n",
    "        \n",
    "        next_state = torch.Tensor(pm.x_test[(t+1)])\n",
    "        next_state.unsqueeze(0)\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            print('Episode Done')\n",
    "            break\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "print('Complete')\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Sell\n",
      "Hold\n",
      "Sell\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Sell\n",
      "Hold\n",
      "Sell\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Sell\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Sell\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Sell\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Sell\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n",
      "Hold\n"
     ]
    }
   ],
   "source": [
    "action_index = ['Buy', 'Sell', 'Hold']\n",
    "for i in range(0,100):\n",
    "    action = target_net(Variable(torch.Tensor(pm.x_train[i])))\n",
    "    a = pm.x_train[i]\n",
    "    a = np.append(a, pm.y_train[i])\n",
    "    _, n = torch.max(action, 0)\n",
    "    print(action_index[n.data[0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-80cb90b12faa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_data_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticker_ls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTICKER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "x, y = self.fd.split_data([self.fd.norm_data_ls[self.fd.ticker_ls.index(self.TICKER)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if (action == 0):\n",
    "            #BUY\n",
    "            if (((next_price - cur_price) > 0) and (since_buy == -1) and (price_buy == -1)):\n",
    "                reward = (next_price - cur_price)*20\n",
    "                price_buy = cur_price\n",
    "                since_buy = 1\n",
    "            elif (((next_price - cur_price) <= 0) and (since_buy == -1) and (price_buy == -1)):\n",
    "                reward = (next_price - cur_price)*20\n",
    "                price_buy = cur_price\n",
    "                since_buy = 1\n",
    "            else:\n",
    "                reward = 0\n",
    "                price_buy = cur_price\n",
    "                since_buy = 1\n",
    "\n",
    "        elif (action == 1):\n",
    "            #SELL\n",
    "            if ((since_buy > 0) & (price_buy > 0)):\n",
    "                reward = (cur_price - price_buy)*100\n",
    "                price_buy = -1\n",
    "                since_buy = -1\n",
    "            else:\n",
    "                reward = 0\n",
    "                price_buy = -1\n",
    "                since_buy = -1\n",
    "\n",
    "\n",
    "\n",
    "        elif (action == 2):\n",
    "            #HOLD\n",
    "            if (((next_price - cur_price) > 0) and (since_buy == -1) and (price_buy == -1)):\n",
    "                reward = -10\n",
    "                price_buy = -1.0\n",
    "                since_buy = -1.0\n",
    "\n",
    "            elif (((next_price - cur_price) <= 0) and (since_buy == -1) and (price_buy == -1)):\n",
    "                reward = 10\n",
    "                price_buy = -1.0\n",
    "                since_buy = -1.0\n",
    "\n",
    "            elif (((next_price - cur_price) > 0) and (since_buy > 0)):\n",
    "                reward = 10*since_buy + (cur_price - price_buy)\n",
    "                price_buy = price_buy\n",
    "                since_buy = since_buy + 1\n",
    "\n",
    "            elif (((next_price - cur_price) < 0) and (since_buy > 0)):\n",
    "                reward = -2*since_buy + (cur_price - price_buy)\n",
    "                price_buy = price_buy\n",
    "                since_buy = since_buy + 1\n",
    "\n",
    "            else:\n",
    "                reward = -100\n",
    "                price_buy = price_buy\n",
    "                since_buy = since_buy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 484,
   "position": {
    "height": "40px",
    "left": "843px",
    "right": "20px",
    "top": "2px",
    "width": "583px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
