{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN in Pytorch\n",
    "Defining different metrics that define a [candle stick](https://stock-market-forecast.blogspot.com/2012/02/automated-recognition-of-candlestick.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plot\n",
    "import time\n",
    "import math\n",
    "import quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_val=quandl.get(\"BCHARTS/BITFINEXUSD\")\n",
    "RB = 100.0 * (stock_val.Close - stock_val.Open) / (stock_val.Open)\n",
    "US = 100.0 * (stock_val.Close - stock_val.Open) / (stock_val.High - stock_val.Open)\n",
    "LS = 100.0 * (stock_val.Close - stock_val.Open) / (stock_val.Close - stock_val.Low)\n",
    "data = torch.zeros((RB.size,3))\n",
    "data[:,0] = torch.from_numpy(RB.values)\n",
    "data[:,1] = torch.from_numpy(US.values)\n",
    "data[:,2] = torch.from_numpy(LS.values)\n",
    "inputdata = torch.Tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Neurual Network model with 3 input features, 2 hidden layers and one output layer.![alt text](https://ml4a.github.io/images/figures/neural-net.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in, H, D_out = 3, 2, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define RNN module with 10 previous outputs to get the next output. RNN module ![alt text](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size, hidden_size, output_size = 100, 128, 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputTensor(x, index, input_size):\n",
    "    inputTensor = x[0, index:(index+input_size)].unsqueeze(0)\n",
    "    return inputTensor\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input.unsqueeze(0), hidden),1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1,self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Two Layer NN and RNN togther for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_size, hidden_size, output_size)\n",
    "learning_rate = 0.0005\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), learning_rate)\n",
    "x = Variable(torch.Tensor(RB.values)).unsqueeze(0)\n",
    "y = Variable(torch.Tensor(RB[2:len(RB)].values)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, output_tensor):\n",
    "    loss = 0\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(input_tensor.size()[0]):\n",
    "        output, hidden = rnn(input_tensor[i], hidden)\n",
    "        loss += criterion(output, output_tensor[i])\n",
    "        optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "#     for p in rnn.parameters():\n",
    "#         p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.data[0] / input_tensor.size()[0]\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 0s (0 0%) 218.3377\n",
      "0m 0s (100 10%) 21.6333\n",
      "0m 0s (200 20%) 59.8745\n",
      "0m 0s (300 30%) 61.3775\n",
      "0m 0s (400 40%) 5.8834\n",
      "0m 0s (500 50%) 7.1114\n",
      "0m 0s (600 60%) 20.1230\n",
      "0m 0s (700 70%) 5.4103\n",
      "0m 0s (800 80%) 0.3697\n",
      "0m 0s (900 90%) 0.5287\n",
      "0m 0s (1000 100%) 0.1403\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "n_iters = 1000\n",
    "print_every = 100\n",
    "plot_every = 100\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(0, n_iters + 1):\n",
    "    output, loss = train(getInputTensor(x, iter, input_size), y[0, iter].unsqueeze(0))\n",
    "    total_loss += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
